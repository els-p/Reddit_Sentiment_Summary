{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit thread to scrape\n",
    "thread = 'Crypto_com' # replace with reddit thread to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/z4f3cmx92fq_bfk_7vjws9d00000gp/T/ipykernel_62699/781600557.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(5)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414197e8ff0a4b559e476afc4314429e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scrapped 127 unique posts\n"
     ]
    }
   ],
   "source": [
    "# Scraper \n",
    "\n",
    "header = {'User-agent': 'ep 0.1.1'}\n",
    "\n",
    "# Set empty list to store posts\n",
    "posts = []\n",
    "\n",
    "# Set param as none for first iteration\n",
    "after = None\n",
    "\n",
    "# Iterate through 5 pages of 25 posts \n",
    "for i in tqdm(range(5)):\n",
    "    if after == None:\n",
    "        param = {}\n",
    "    else:\n",
    "        param = {'after': after}\n",
    "    url = 'https://www.reddit.com/r/'+thread+'/.json'\n",
    "    results = requests.get(url, params=param, headers=header)\n",
    "    if results.status_code == 200: # Check if request successful\n",
    "        res_json = results.json()\n",
    "        posts.extend(res_json['data']['children'])\n",
    "        after = res_json['data']['after']\n",
    "    else:\n",
    "        print(results.status_code)\n",
    "        break\n",
    "    #  Rest time in seconds\n",
    "    time.sleep(1)\n",
    "\n",
    "# Records only posts from unique users\n",
    "posts = pd.DataFrame(posts)\n",
    "lst = {}\n",
    "lst['post_title'] =[]\n",
    "lst['content'] =[]\n",
    "lst['name'] =[]\n",
    "for i in posts['data']:\n",
    "    if i['name'] not in lst['name']:\n",
    "        lst['post_title'].append(i['title'])\n",
    "        lst['content'].append(i['selftext'])\n",
    "        lst['name'].append(i['name'])\n",
    "\n",
    "print('Successfully scrapped {} unique posts'.format(len(lst['post_title'])))\n",
    "scrapped_thread = pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset \n",
    "file_name='./'+thread+'_thread.csv'\n",
    "scrapped_thread.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Delayed Employer Direct Deposit via ACH to Fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>How to get ETH on the Ethereum chain from ETH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Received Exclusive Merchandise Welcome Pack af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Lounge access after may 31th 2024 -- Hi,\\n\\nth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Closest Thing To A Brokerage Statement From CD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Help getting account back -- I‚Äôve fallen on ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Why is the price like this? -- So when i took ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TUSD at 155% APY?? -- Hello all,\\n\\nI am check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>People who managed to create a good grid bot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What happened to moonüåõMOONNODE???? 0% p.a. All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Missing EOS deposit -- I made a deposit to my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Exchange gone -- smart move or trying to hide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crypto_com New T&amp;amp;C -- There was a new term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Is this new? Since when is there a spread for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Error on earn page -- Is anyone else seeing this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>desktop DEFI wallet self destruct -- hi i've f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Error message -- Hey all! I‚Äôm\\nGetting an erro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The metal card has to have some residual value...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Thanks for clearing that üëç -- Icy is unlimited.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Receive 8% for rose gold holder -- How is 8% c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title_content\n",
       "24  Delayed Employer Direct Deposit via ACH to Fia...\n",
       "41  How to get ETH on the Ethereum chain from ETH ...\n",
       "28  Received Exclusive Merchandise Welcome Pack af...\n",
       "58  Lounge access after may 31th 2024 -- Hi,\\n\\nth...\n",
       "31  Closest Thing To A Brokerage Statement From CD...\n",
       "43  Help getting account back -- I‚Äôve fallen on ha...\n",
       "44  Why is the price like this? -- So when i took ...\n",
       "52  TUSD at 155% APY?? -- Hello all,\\n\\nI am check...\n",
       "11  People who managed to create a good grid bot, ...\n",
       "12  What happened to moonüåõMOONNODE???? 0% p.a. All...\n",
       "49  Missing EOS deposit -- I made a deposit to my ...\n",
       "34  Exchange gone -- smart move or trying to hide ...\n",
       "1   Crypto_com New T&amp;C -- There was a new term...\n",
       "18  Is this new? Since when is there a spread for ...\n",
       "68  Error on earn page -- Is anyone else seeing this?\n",
       "45  desktop DEFI wallet self destruct -- hi i've f...\n",
       "72  Error message -- Hey all! I‚Äôm\\nGetting an erro...\n",
       "40  The metal card has to have some residual value...\n",
       "60    Thanks for clearing that üëç -- Icy is unlimited.\n",
       "56  Receive 8% for rose gold holder -- How is 8% c..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df_thread = pd.read_csv(file_name)\n",
    "df_title_content = pd.DataFrame(df_thread.post_title + ' -- ' + df_thread.content, columns = [\"title_content\"]).dropna().reset_index(drop=True)\n",
    "\n",
    "# Data processing \n",
    "# Remove rows with attachments\n",
    "df_title_content = df_title_content[\n",
    "                                        (df_title_content.title_content.str.contains(\"png\")==False)&\n",
    "                                        (df_title_content.title_content.str.contains(\"ampx200b\\n\\nhttpsredditcom\")==False)&\n",
    "                                        (df_title_content.title_content.str.contains(\"&amp;#x200B\")==False)\n",
    "                                    ]\n",
    "df_title_content = df_title_content.reset_index(drop=True)\n",
    "df_title_content.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sentiment model from hugging face: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "# Import libraries \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "# now = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model and defined functions to get sentiment of imported reddit data\n",
    "sentiment=[]\n",
    "for i in df_title_content.title_content:\n",
    "    text = preprocess(i)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    sentiment.append(labels[ranking[0]])\n",
    "\n",
    "df_title_content['sentiment']=sentiment\n",
    "\n",
    "# Save post and sentiment to file\n",
    "df_title_content.to_csv(thread+'_clean_content_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts retrieved: 76\n"
     ]
    }
   ],
   "source": [
    "# Check number of posts retrieved\n",
    "print(\"Number of posts retrieved: \"+str(df_title_content.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use openai API to summarise the dataset\n",
    "\n",
    "# Import library\n",
    "import openai\n",
    "\n",
    "def summarize_corpus(corpus):\n",
    "    # Set up OpenAI API credentials\n",
    "    openai.api_key = 'YOUR_API_KEY' #replace with own key\n",
    "\n",
    "    # Provide the prompt and settings for the API call\n",
    "    prompt = 'Summarize the following text: ' + corpus\n",
    "    max_tokens = 500  # Maximum number of tokens for the summary\n",
    "\n",
    "    # Call the OpenAI API to generate the summary\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.2,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    # Extract the generated summary from the API response\n",
    "    summary = response.choices[0].text.strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Use nltk to gauge token count\n",
    "\n",
    "# Import library\n",
    "import nltk\n",
    "\n",
    "def count_tokens(corpus):\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    token_count = len(tokens)\n",
    "    return token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "negative    37\n",
      "neutral     32\n",
      "positive     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of negative posts used: 30\n",
      "This post is about people having issues with Crypto.com, such as not being able to pay friends, withdraw funds, transfer money, and access their accounts. People are also complaining about the bonuses, rewards, customer support, and fees associated with the platform. They are also asking questions about how to use the platform, such as how to withdraw coins to a connected defi wallet, how to unlock a card after entering the wrong pin, and if PayID payments have been blocked in Australia.\n",
      "\n",
      "Number of neutral posts used: 32\n",
      "This text contains a variety of questions and comments related to Crypto.com. Questions include: delisting $PEPE, BTC withdrawal fees, network importance, receiving 8% for Rose Gold holders, Priority Pass and guests, benefits of holding a Ruby Card, staking on the Cosmos system, Supercharger calculations, time to unstake ATOM and CRO in DeFi, Priority Pass validity, and fees including spread fee.\n",
      "\n",
      "Number of positive posts used: 7\n",
      "This text discusses the changes to Crypto.com's lounge access service, which previously allowed unlimited access but has now been limited to four visits for Indigo/Jade cardholders. The author supports this decision, arguing that it is economically and rationally responsible and allows Crypto.com to optimize their offerings and ensure long-term sustainability. They also point out that four visits is still a great benefit, as many other services or programs might not offer any lounge access at all.\n"
     ]
    }
   ],
   "source": [
    "# Apply defined functions\n",
    "print(df_title_content.sentiment.value_counts())\n",
    "for n,k in enumerate(labels):\n",
    "    corpus = df_title_content[df_title_content.sentiment==str(k)].title_content\n",
    "    if count_tokens(corpus.to_json()) > 3000: #estimate prompt max token count to be 3000 excluding 500 tokens for output\n",
    "        corpus = corpus.iloc[:30]\n",
    "        summary = summarize_corpus(corpus.to_json())\n",
    "        num_posts = corpus.shape[0]\n",
    "    else:\n",
    "        summary = summarize_corpus(corpus.to_json())\n",
    "        num_posts = corpus.shape[0]\n",
    "    \n",
    "    print('\\n'\"Number of \"+str(k)+\" posts used:\", num_posts)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
